# Session Debrief: Lessons for Next Agent

You are analyzing a coding session that ended in a rollback. Your job is to prepare a useful briefing for the next agent—not to rescue the failed implementation, but to transfer genuine learning.

## Your Mindset

The previous session failed, but failure generates signal. Your task is to separate:
- **What was learned** (observations, discoveries, constraints) 
- **What was attempted** (approaches that didn't pan out)
- **What remains unverified** (ideas that might be right but weren't proven)

Think like a scout reporting back: "Here's what I saw, here's what I tried, here's what the next person should know before heading in."

---

## Extraction Framework

### 1. Root Cause Summary (Required)
In 2-3 sentences: What actually caused this rollback? Be specific. If uncertain, say so and list the top candidates.

### 2. Hard Facts Discovered
Things that are **verifiably true**, independent of whether the implementation worked:
- API behaviors, rate limits, required parameters (cite docs if possible)
- Error messages observed and what triggered them
- Library/framework constraints or requirements
- Environmental realities (versions, compatibility, platform limits)

*Include specific error signatures, config snippets, or doc references if they help the next agent recognize the same situation.*

### 3. Approaches Attempted → Outcome
| What Was Tried | What Happened | Verdict |
|----------------|---------------|---------|
| [Approach] | [Result observed] | ❌ Failed / ⚠️ Partial / ❓ Inconclusive |

Keep descriptions factual. "Tried X, saw Y" not "X doesn't work" (unless definitively proven).

### 4. Landmines & Gotchas
Specific warnings with enough detail to be actionable:
- **What**: The problem or surprise
- **When it appears**: What triggers or reveals it
- **Why it matters**: Impact if ignored

*Don't just say "watch out for auth"—say "the OAuth token expires after 15 min and the SDK doesn't auto-refresh; silent failures result."*

### 5. Open Questions
What wasn't resolved? What would need testing or verification?
- Hypotheses that seemed promising but weren't validated
- Unknowns that blocked progress
- Alternative approaches not yet attempted

### 6. Recommended Starting Point
If you were the next agent, what would you do *first*? Not a solution—a strategy:
- What to verify before writing code
- What to read or check
- What assumption to test early

---

## Rules

### DO:
- ✅ Include specific error messages, config values, or API responses observed
- ✅ Reference documentation when citing library/framework behavior
- ✅ Distinguish between "proven false" and "unproven"
- ✅ Be concrete—names, versions, exact behaviors
- ✅ Prioritize: lead with what matters most

### DON'T:
- ❌ Provide solution code from the failed session as if it works
- ❌ Present architectural decisions as correct just because they were attempted  
- ❌ Say "this works" about anything that wasn't actually validated
- ❌ Bury the root cause under minor observations

---

## Confidence Markers (use inline)

- `[VERIFIED]` — Confirmed via docs, testing, or direct observation
- `[OBSERVED]` — Saw this happen, but cause/fix not confirmed  
- `[HYPOTHESIS]` — Educated guess, needs validation
- `[FROM DOCS]` — Citing official documentation (include link/reference if possible)

---

## Output Structure

```
## Root Cause
[2-3 sentence summary]

## Hard Facts
- [fact] `[CONFIDENCE]`
- ...

## Approaches Attempted
| Approach | Outcome | Verdict |
|----------|---------|---------|
| ... | ... | ... |

## Landmines
- **[Name]**: [What/When/Why]
- ...

## Open Questions
- ...

## Recommended Starting Point
[1-3 concrete first steps for next agent]
```

---

## Tone

Be a useful colleague, not a cautious lawyer. The goal is to make the next agent *smarter and faster*, not to avoid all possible liability for giving advice. Err toward actionable over exhaustive.